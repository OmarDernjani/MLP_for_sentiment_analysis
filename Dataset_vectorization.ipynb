{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34a7c2fd-cfd6-4067-8f27-571e5e1f8894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kagglehub\n",
      "  Downloading kagglehub-0.3.13-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\dernj\\desktop\\mlp_sent\\mlp_env\\lib\\site-packages (from kagglehub) (25.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\dernj\\desktop\\mlp_sent\\mlp_env\\lib\\site-packages (from kagglehub) (6.0.3)\n",
      "Requirement already satisfied: requests in c:\\users\\dernj\\desktop\\mlp_sent\\mlp_env\\lib\\site-packages (from kagglehub) (2.32.5)\n",
      "Collecting tqdm (from kagglehub)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\dernj\\desktop\\mlp_sent\\mlp_env\\lib\\site-packages (from requests->kagglehub) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dernj\\desktop\\mlp_sent\\mlp_env\\lib\\site-packages (from requests->kagglehub) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dernj\\desktop\\mlp_sent\\mlp_env\\lib\\site-packages (from requests->kagglehub) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dernj\\desktop\\mlp_sent\\mlp_env\\lib\\site-packages (from requests->kagglehub) (2025.11.12)\n",
      "Requirement already satisfied: colorama in c:\\users\\dernj\\desktop\\mlp_sent\\mlp_env\\lib\\site-packages (from tqdm->kagglehub) (0.4.6)\n",
      "Downloading kagglehub-0.3.13-py3-none-any.whl (68 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, kagglehub\n",
      "\n",
      "   ---------------------------------------- 0/2 [tqdm]\n",
      "   ---------------------------------------- 0/2 [tqdm]\n",
      "   ---------------------------------------- 0/2 [tqdm]\n",
      "   ---------------------------------------- 0/2 [tqdm]\n",
      "   ---------------------------------------- 0/2 [tqdm]\n",
      "   -------------------- ------------------- 1/2 [kagglehub]\n",
      "   -------------------- ------------------- 1/2 [kagglehub]\n",
      "   -------------------- ------------------- 1/2 [kagglehub]\n",
      "   -------------------- ------------------- 1/2 [kagglehub]\n",
      "   ---------------------------------------- 2/2 [kagglehub]\n",
      "\n",
      "Successfully installed kagglehub-0.3.13 tqdm-4.67.1\n"
     ]
    }
   ],
   "source": [
    "!pip install kagglehub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9431f07f-53a2-408a-a4c8-2b90f34d9cbe",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0ae57de9-b42b-47d7-ac01-cef7a2dda889",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.sparse import save_npz\n",
    "import seaborn as sns\n",
    "import kagglehub\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209f1e98-a055-45bc-8689-1c8891056d56",
   "metadata": {},
   "source": [
    "# Dataset Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dba972e4-76da-46d2-b091-188aaa759cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/kazanova/sentiment140?dataset_version_number=2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 80.9M/80.9M [00:14<00:00, 5.98MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\dernj\\.cache\\kagglehub\\datasets\\kazanova\\sentiment140\\versions\\2\n"
     ]
    }
   ],
   "source": [
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"kazanova/sentiment140\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d65afb9-cb40-431d-9b6e-05e0a471ac17",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(path)\n",
    "df = pd.read_csv(path +  '/' + os.listdir(path)[0], encoding = 'latin-1', names = ['target','ids','date','flag','user','text'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f37574d-4a73-4e35-8110-9967cdec874a",
   "metadata": {},
   "source": [
    "About Dataset\n",
    "# Context\n",
    "This is the sentiment140 dataset. It contains 1,600,000 tweets extracted using the twitter api . The tweets have been annotated (0 = negative, 4 = positive) and they can be used to detect sentiment .\n",
    "\n",
    "# Content\n",
    "It contains the following 6 fields:\n",
    "\n",
    "target: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
    "\n",
    "ids: The id of the tweet ( 2087)\n",
    "\n",
    "date: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
    "\n",
    "flag: The query (lyx). If there is no query, then this value is NO_QUERY.\n",
    "\n",
    "user: the user that tweeted (robotickilldozr)\n",
    "\n",
    "text: the text of the tweet (Lyx is cool)\n",
    "\n",
    "# Acknowledgements\n",
    "The official link regarding the dataset with resources about how it was generated is here\n",
    "The official paper detailing the approach is here\n",
    "\n",
    "Citation: Go, A., Bhayani, R. and Huang, L., 2009. Twitter sentiment classification using distant supervision. CS224N Project Report, Stanford, 1(2009), p.12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32268a59-1df8-45ae-a573-c003f40282a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target         ids                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "508dc434-d901-4d54-b140-33565e19e21b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    800000\n",
       "4    800000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = df['target']\n",
    "target.head()\n",
    "target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f982d00-ca5a-49a7-9e6f-c1f14b85ff56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1    is upset that he can't update his Facebook by ...\n",
       "2    @Kenichan I dived many times for the ball. Man...\n",
       "3      my whole body feels itchy and like its on fire \n",
       "4    @nationwideclass no, it's not behaving at all....\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = df['text']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425f755e-56e0-4a08-a77a-04b4fc6a2680",
   "metadata": {},
   "source": [
    "# Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2c71f09d-10a0-4629-aa80-9e5ec5509fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset to prevent data leakage \n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, random_state = 0, test_size = 0.3)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, random_state = 0, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dc696b54-a4a8-4063-9ad9-af2db697c1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup the vectorizer for the dataset\n",
    "bin_vec = CountVectorizer(binary = True)\n",
    "freq_vec = CountVectorizer(binary = False)\n",
    "tfidf_vec = TfidfVectorizer(max_features = 40000)\n",
    "hash_vec = HashingVectorizer()\n",
    "\n",
    "#CountVectorizer with binary = True\n",
    "data_bin_train = bin_vec.fit_transform(X_train)\n",
    "data_bin_test = bin_vec.transform(X_test)\n",
    "data_bin_val = bin_vec.transform(X_val)\n",
    "\n",
    "#CountVectorizer with binary = False\n",
    "data_freq_train = freq_vec.fit_transform(X_train)\n",
    "data_freq_test = freq_vec.transform(X_test)\n",
    "data_freq_val = freq_vec.transform(X_val)\n",
    "\n",
    "#TfidfVectorizer truncated\n",
    "data_tfidf_train = tfidf_vec.fit_transform(X_train)\n",
    "data_tfidf_test = tfidf_vec.transform(X_test)\n",
    "data_tfidf_val = tfidf_vec.transform(X_val)\n",
    "\n",
    "#HashingVectorizer\n",
    "data_hash_train = hash_vec.fit_transform(X_train)\n",
    "data_hash_test = hash_vec.transform(X_test)\n",
    "data_hash_val = hash_vec.transform(X_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4b6a22-ee4a-4206-a243-073b5d2bce74",
   "metadata": {},
   "source": [
    "# Save the vectorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ea0c7898-5f76-4480-9d3b-f227ceceb90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the vectorized dataset\n",
    "vectorizers = {\n",
    "    'bin_vec': bin_vec,\n",
    "    'freq_vec': freq_vec,\n",
    "    'tfidf_vec': tfidf_vec,\n",
    "    'hash_vec': hash_vec\n",
    "}\n",
    "\n",
    "with open('vectorizers.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorizers, f)\n",
    "\n",
    "# Binary\n",
    "save_npz('data_bin_train.npz', data_bin_train)\n",
    "save_npz('data_bin_test.npz', data_bin_test)\n",
    "save_npz('data_bin_val.npz', data_bin_val)\n",
    "\n",
    "# Frequency\n",
    "save_npz('data_freq_train.npz', data_freq_train)\n",
    "save_npz('data_freq_test.npz', data_freq_test)\n",
    "save_npz('data_freq_val.npz', data_freq_val)\n",
    "\n",
    "# TF-IDF\n",
    "save_npz('data_tfidf_train.npz', data_tfidf_train)\n",
    "save_npz('data_tfidf_test.npz', data_tfidf_test)\n",
    "save_npz('data_tfidf_val.npz', data_tfidf_val)\n",
    "\n",
    "# Hashing\n",
    "save_npz('data_hash_train.npz', data_hash_train)\n",
    "save_npz('data_hash_test.npz', data_hash_test)\n",
    "save_npz('data_hash_val.npz', data_hash_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
